---
title: "Learning statistics with R: A tutorial for psychology students and other beginners"
subtitle: "(Version 0.6)"
author: "Fernando Millan Villalobos"
date: "April 2021"
output:
  html_document:
    code_folding: show
    echo: TRUE
    warning: FALSE
    message: FALSE
    highlight: pygments
    theme: paper
    df_print: kable
    toc: yes
    toc_depth: 4
    number_sections: yes
    toc_float: 
      collapsed: yes
      smooth_scroll: false
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../", output_file = "index") })   
---

```{r, include=FALSE}
## By default, show code for all chunks in the knitted document,
## as well as the output. To override for a particular chunk
## use echo = FALSE in its options.
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE
)
```

```{r, echo=FALSE}
# CONFIG
user_name <- "fernandomillanvillalobos" # your Git username (only needed if
# you want to deploy to GH pages)
project_name <- "rddj-template" # adapt!
package_date <- "2021-01-01" # date of the CRAN snapshot that
# the checkpoint package uses
r_version <- "4.1.2" # R-Version to use
options(Ncpus = 4) # use 4 cores for parallelized installation of packages
if (r_version != paste0(version$major, ".", version$minor)) {
  stop("ERROR: specified R version does not match currently used.")
}
```

# Notes

This report was generated on `r Sys.time()`. R version: `r paste0(version$major, ".", version$minor)` on `r version$platform`. For this report, CRAN packages as of `r package_date` were used.

...

## R-Script & data

The preprocessing and analysis of the data was conducted in the [R project for statistical computing](https://www.r-project.org/). The RMarkdown script used to generate this document and all the resulting data can be downloaded [under this link](http://%60r%20user_name%60.github.io/%60r%20project_name%60/). Through executing `main.Rmd`, the herein described process can be reproduced and this document can be generated. In the course of this, data from the folder `input` will be processed and results will be written to `output`. The html on-line version of the analysis can be accessed through this [link](https://%60r%20user_name%60.github.io/%60r%20project_name%60/).

## GitHub

The code for the herein described process can also be freely downloaded from [https://github.com/`r user_name`/`r project_name`](https://github.com/%60r%20user_name%60/%60r%20project_name%60).

## License

...

## Data description of output files

#### `abc.csv` (Example)

| Attribute | Type    | Description |
|-----------|---------|-------------|
| a         | Numeric | ...         |
| b         | Numeric | ...         |
| c         | Numeric | ...         |

#### `xyz.csv`

...

# Set up

```{r, echo=FALSE}
detach_all_packages <- function() {
  basic_packages_blank <- c(
    "stats",
    "graphics",
    "grDevices",
    "utils",
    "datasets",
    "methods",
    "base"
  )
  basic_packages <- paste("package:", basic_packages_blank, sep = "")

  package_list <- search()[
    ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)
  ]

  package_list <- setdiff(package_list, basic_packages)

  if (length(package_list) > 0) {
    for (package in package_list) {
      detach(package, character.only = TRUE, unload = TRUE)
      print(paste("package ", package, " detached", sep = ""))
    }
  }
}

detach_all_packages()

# this allows multiple persons to use the same RMarkdown
# without adjusting the working directory by themselves all the time
source("scripts/csf.R")
path_to_wd <- csf() # if this - for some reason - does not work,
# replace with a hardcoded path, like so: "~/projects/rddj-template/analysis/"
if (is.null(path_to_wd) | !dir.exists(path_to_wd)) {
  print("WARNING: No working directory specified for current user")
} else {
  setwd(path_to_wd)
}

# suppress scientific notation
options(scipen = 999)

# suppress summarise info
options(dplyr.summarise.inform = FALSE)

# unload global rstudioapi and knitr again to avoid conflicts with checkpoint
# this is only necessary if executed within RStudio
# outside of RStudio, namely in the knit.sh script, this causes RMarkdown
# rendering to fail, thus should not be executed there
if (Sys.getenv("RSTUDIO") == "1") {
  detach_all_packages()
}
```

## Define packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# from https://mran.revolutionanalytics.com/web/packages/\
# checkpoint/vignettes/using-checkpoint-with-knitr.html
# if you don't need a package, remove it from here (commenting not sufficient)
# tidyverse: see https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/
cat("
library(rstudioapi)
library(tidyverse, warn.conflicts = FALSE) # ggplot2, dplyr, tidyr, readr, purrr, tibble, magrittr, readxl
library(scales) # scales for ggplot2
library(jsonlite) # json
library(lintr) # code linting
library(rmarkdown)
library(data.table)
library(cowplot) # theme
library(extrafont)
library(waldo) # compare
library(psych) # some useful funs 
library(ggrepel) # text labels
library(skimr)
library(lsr) # book package
library(vcd)
library(vcdExtra)
library(sciplot)
library(gplots)
library(car) # statistics tests 
library(lmtest) # more statistics tests
library(itns) # book itns
library(effects)
library(BayesFactor)
library(janitor)", # names
  file = "manifest.R"
)
```

## Install packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# if checkpoint is not yet installed, install it (for people using this
# system for the first time)
if (!require(checkpoint)) {
  if (!require(devtools)) {
    install.packages("devtools", repos = "http://cran.us.r-project.org")
    require(devtools)
  }
  devtools::install_github("RevolutionAnalytics/checkpoint",
    ref = "v0.3.2", # could be adapted later,
    # as of now (beginning of July 2017
    # this is the current release on CRAN)
    repos = "http://cran.us.r-project.org"
  )
  require(checkpoint)
}
# nolint start
if (!dir.exists("~/.checkpoint")) {
  dir.create("~/.checkpoint")
}
# nolint end
# install packages for the specified CRAN snapshot date
checkpoint(
  snapshot_date = package_date,
  project = path_to_wd,
  verbose = T,
  scanForPackages = T,
  use.knitr = F,
  R.version = r_version
)
rm(package_date)
```

## Load packages

```{r, echo=TRUE, message=FALSE, warning=FALSE}
source("manifest.R")
unlink("manifest.R")
sessionInfo()
```

## Load additional scripts

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# if you want to outsource logic to other script files, see README for
# further information
# Load all visualizations functions as separate scripts
knitr::read_chunk("scripts/dviz.supp.R")
source("scripts/dviz.supp.R")
knitr::read_chunk("scripts/themes.R")
source("scripts/themes.R")
knitr::read_chunk("scripts/plot_grid.R")
source("scripts/plot_grid.R")
knitr::read_chunk("scripts/align_legend.R")
source("scripts/align_legend.R")
knitr::read_chunk("scripts/label_log10.R")
source("scripts/label_log10.R")
```

# Programming

## Loops

```{r loops}
# the while loop
x <- 0
while (x < 1000) {
  x <- x + 17
}
x

# the for loop
for (i in 1:3) {
  print("hello")
}

# a simple example
words <- c("it", "was", "the", "dirty", "end", "of", "winter")
for (w in words) {
  w.length <- nchar(w)
  W <- toupper(w)
  msg <- paste(W, "has", w.length, "letters")
  print(msg)
}

# a more complex example
# set up
month <- 0 # count the number of months
balance <- 300000 # initial mortgage balance
payments <- 1600 # monthly payments
interest <- 0.05 # 5% interest rate per year
total.paid <- 0 # track what you've paid the bank

# convert annual interest to a monthly multiplier
monthly.multiplier <- (1 + interest)^(1 / 12)

# keep looping until the loan is paid off...
while (balance > 0) {

  # do the calculations for this month
  month <- month + 1 # one more month
  balance <- balance * monthly.multiplier # add the interest
  balance <- balance - payments # make the payments
  total.paid <- total.paid + payments # track the total paid

  # print the results on screen
  cat("month", month, ": balance", round(balance), "\n")
} # end of loop

# print the total payments at the end
cat("total payments made", total.paid, "\n")
```
## Conditional statements

```{r conditional_statements}
# find out what day it is...
today <- Sys.Date() # pull the date from the system clock
day <- weekdays(today) # what day of the week it is_

# now make a choice depending on the day...
if (day == "Monday") {
  print("I don't like Mondays")
} else {
  print("I'm a happy little automaton")
}
```
## Functions
A formal argument can be a symbol (i.e. a variable name such as x or y), a statement of the form symbol = expression (e.g. pch=16) or the special formal argument ... (triple dot).

```{r functions}
# a simple function
quadruple <- function(x) {
  y <- x * 4
  return(y)
}
quadruple(3)

# another simple function with two arguments
pow <- function(x, y = 1) { # setting a default value for args
  out <- x^y # raise x to the power y
  return(out)
}
pow(3, 2)

# using the ... argument
doubleMax <- function(...) {
  max.val <- max(...) # find the largest value in ...
  out <- 2 * max.val # double it
  return(out)
}
doubleMax(4, 6, 9)
```

## Implicit loops

```{r implicit_loops}
# sapply()
words <- c("along", "the", "loom", "of", "the", "land")
sapply(words, FUN = nchar)

# tapply()
gender <- c("male", "male", "female", "female", "male")
age <- c(10, 12, 9, 11, 13)
# loop over all values that appear in the INDEX grouping variable
tapply(age, INDEX = gender, FUN = mean)

# by
by(age, INDICES = gender, FUN = mean) # does the same as tapply
```

# Descriptive statistics

## Measures of central tendency

```{r central_tendency}
# Load data
load("~/lsr/lsr/analysis/input/aflsmall.Rdata")
table(afl.finalists)

# calculate the mode
m <- modeOf(afl.finalists)
cat("The mode of the dataset is", m, "\n")

# calculate the modal frequency
max <- maxFreq(afl.finalists)
cat("The modal frequency of the dataset is", max, "\n")
```

## Measures of variability

```{r variability}
# calculate quantiles
quantile(afl.margins, probs = .5)
quantile(afl.margins, probs = c(.25, .75))

# calculate IQR
i <- IQR(afl.margins)
cat("IQR is", i, "\n")

# calculate the mean absolute deviation (AAD)
a <- lsr::aad(afl.margins[1:5])
cat("The mean absolute deviation (AAD) is", a, "\n")

# calculate variance (also called mean square deviation)
v <- var(afl.margins[1:5])
cat("The variance is", v, "\n")

# calculate standard deviation (also called root mean squared deviation)
s <- sd(afl.margins)
cat("The standard deviation is", s, "\n")

# calculate median absolute deviation (MAD)
m <- mad(afl.margins, constant = 1)
cat("The median absolute deviation (MAD) is", m, "\n")

# calculate skewness
sk <- psych::skew(afl.margins)
cat("The skewness is", sk, "\n")

# calculate kurtosis
k <- psych::kurtosi(afl.margins)
cat("The kurtosis is", k, "\n")
```

## Overall data summary

```{r data_summary}
# load data
load("~/lsr/lsr/analysis/input/clinicaltrial.Rdata")

# for numeric variables
summary(afl.margins)

# for nominal (factor) variables
summary(afl.finalists)

# for data frames
psych::describe(x = clin.trial)

# broken down by grouping variable
psych::describeBy(clin.trial, group = clin.trial$therapy)
by(clin.trial, INDICES = clin.trial$therapy, FUN = describe)
by(clin.trial, INDICES = clin.trial$therapy, FUN = summary)
aggregate(mood.gain ~ drug + therapy, data = clin.trial, FUN = mean)
```

## Correlations

```{r correlations}
# load data
load("~/lsr/lsr/analysis/input/parenthood.Rdata")

hist(parenthood$dan.grump)
hist(parenthood$dan.sleep)
hist(parenthood$baby.sleep)

plot(x = parenthood$dan.sleep, y = parenthood$dan.grump)
plot(x = parenthood$baby.sleep, y = parenthood$dan.grump)

# calculate correlations (the Pearson's correlation)
cor(x = parenthood$dan.sleep, y = parenthood$dan.grump)
cor(parenthood)

# load data
load("~/lsr/lsr/analysis/input/effort.Rdata")

# calculate correlations (the Spearmans's correlation)
hours_rank <- rank(effort$hours)
grade_rank <- rank(effort$grade)
cor(hours_rank, grade_rank)
# or
cor(effort$hours, effort$grade, method = "spearman")

# calculate correlations getting rid of all non-numeric variables
# # load data
load("~/lsr/lsr/analysis/input/work.Rdata")

lsr::correlate(work)
lsr::correlate(work, corr.method = "spearman")

# look at the data
head(body_well)
dplyr::glimpse(body_well)

plot(x = body_well$bodysat, y = body_well$wellbeing)

ggplot(body_well, aes(x = bodysat, y = wellbeing)) +
  geom_point(shape = 21, fill = "deepskyblue4", stroke = .5, size = 3, alpha = .7) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_cowplot()

# calculating Pearson's correlation coefficient
cor(body_well$bodysat, body_well$wellbeing, method = "spearman")
lsr::correlate(x = body_well$bodysat, y = body_well$wellbeing, corr.method = "spearman")


# eyeballing r correlation coefficient
ggplot(body_well, aes(x = bodysat, y = wellbeing)) +
  geom_point(shape = 21, fill = "deepskyblue4", stroke = .5, size = 3, alpha = .7) +
  geom_vline(aes(xintercept = mean(bodysat)), size = .3, linetype = "dashed", color = "red") +
  geom_hline(aes(yintercept = mean(wellbeing)), size = .3, linetype = "dashed", color = "red") +
  theme_cowplot()

# look at the data
head(thomason1)
dplyr::glimpse(thomason1)

plot(x = thomason1$pre, y = thomason1$post)

ggplot(thomason1, aes(x = pre, y = post)) +
  geom_point(shape = 21, fill = "deepskyblue4", stroke = .5, size = 3, alpha = .7) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 20), breaks = seq(0, 20, 2)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 18), breaks = seq(0, 18, 2)) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_cowplot()

# calculating Pearson's correlation coefficient
cor(thomason1$pre, thomason1$post, method = "spearman")
lsr::correlate(x = thomason1$pre, y = thomason1$post, corr.method = "spearman")

# eyeballing r correlation coefficient
ggplot(thomason1, aes(x = pre, y = post)) +
  geom_point(shape = 21, fill = "deepskyblue4", stroke = .5, size = 3, alpha = .7) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 20), breaks = seq(0, 20, 2)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 18), breaks = seq(0, 18, 2)) +
  geom_vline(aes(xintercept = mean(pre)), size = .3, linetype = "dashed", color = "red") +
  geom_hline(aes(yintercept = mean(post)), size = .3, linetype = "dashed", color = "red") +
  theme_cowplot()

# look at the data
head(sleep_beauty)
dplyr::glimpse(sleep_beauty)

# eyeballing r correlation coefficient
ggplot(sleep_beauty, aes(x = nightly_sleep_hours, y = rated_attractiveness)) +
  geom_point(shape = 21, fill = "deepskyblue4", stroke = .5, size = 3, alpha = .7) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 8), breaks = seq(0, 8, 1)) +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 14), breaks = seq(0, 14, 1)) +
  geom_vline(aes(xintercept = mean(nightly_sleep_hours)), size = .3, linetype = "dashed", color = "red") +
  geom_hline(aes(yintercept = mean(rated_attractiveness)), size = .3, linetype = "dashed", color = "red") +
  theme_cowplot()

# calculating CI's for r
cor.test(x = sleep_beauty$nightly_sleep_hours, y = sleep_beauty$rated_attractiveness)
```

## Handling missing values

```{r correlations_missing}
# load data
load("~/lsr/lsr/analysis/input/parenthood2.Rdata")

# ignore all cases that have any missing values
cor(parenthood2, use = "complete.obs")

# look at the variables when determining what to drop
cor(parenthood2, use = "pairwise.complete.obs")
```

## Transforming or recoding a variable

```{r transforming}
# load data
load("~/lsr/lsr/analysis/input/likert.Rdata")

# make a data frame
df <- data.frame(likert.raw)
# recode the variable (+3 strong agree, 0 no opinion, -3 strong disagree)
df$likert.centred <- df$likert.raw - 4
# create a strength variable
df$opinion.strength <- abs(df$likert.centred)
# create a direction variable (all negative numbers converted to -1, all positives to 1 and zero stays 0)
df$opinion.dir <- sign(df$likert.centred)
df

# cutting numeric variables into categories
age <- c(60, 58, 24, 26, 34, 42, 31, 30, 33, 2, 9)
age.breaks <- seq(from = 0, to = 60, by = 20)
age.labels <- c("young", "adult", "older")
age.group <- cut(x = age, breaks = age.breaks, labels = age.labels)
age.df <- data.frame(age, age.group)
```

## Subsetting

```{r subsetting}
# load data
load("~/lsr/lsr/analysis/input/nightgarden.Rdata")
itng <- data.frame(speaker, utterance)


# using split
speech.by.char <- split(utterance, speaker)
speech.by.char

# pull grouping variables out of the list and into workspace
lsr::importList(speech.by.char, ask = FALSE)

# using subset
df <- subset(itng, subset = speaker == "makka-pakka", select = utterance)
```

## Sorting, flipping and merging data

```{r sorting-flipping-mergin}
# load data
load("~/lsr/lsr/analysis/input/nightgarden2.Rdata")

# sorting a data frame
lsr::sortFrame(garden, speaker, line)

# reverse order
lsr::sortFrame(garden, -speaker, line)
```

## Working with text

```{r text}
# Shortening a string
animals <- c("cat", "dog", "kangaroo", "whale")

strtrim(animals, width = 3)
substr(animals, start = 2, stop = 3)

# Paste strings together
paste("hello", "world")
paste0("hello", "world")
paste("hello", "world", sep = ".")
hw <- c("hello", "world")
ng <- c("nasty", "government")
paste(hw, ng)
paste(hw, ng, collapse = " ")

# Splitting strings
monkey <- "It was the best of times. It was the blurst of times."
monkey.1 <- strsplit(monkey, split = " ", fixed = T)
monkey.2 <- strsplit(monkey, split = ".", fixed = T)

# Making transformations
old.text <- "albmino"
chartr(old = "alb", new = "chu", old.text)

# Matching and substituting text
beers <- c("little creatures", "sierra nevada", "coopers pale")
grep(pattern = "er", beers, fixed = T, value = T)
gsub(pattern = "a", replacement = "BLAH", beers, fixed = T)
sub(pattern = "a", replacement = "BLAH", beers, fixed = T)
```

# Inferential statistics

## Introduction to probability

```{r proba_distributions}
# Functions to draw plots to show probability distributions

# Parameters for plots
emphCol <- rgb(0, 0, 1)
colour <- TRUE
emphColLight <- rgb(.5, .5, 1)
emphGrey <- grey(.5)

# Function to produce a styled binomial plot
binomPlot <- function(n, p, ...) {

  # probabilities of each outcome
  out <- 0:n
  prob <- dbinom(x = out, size = n, prob = p)

  # plot
  plot(
    out, prob,
    type = "h", lwd = 3, ylab = "Probability",
    frame.plot = FALSE, col = ifelse(colour, emphCol, "black"), ...
  )
}
binomPlot(40, 1 / 4, xlab = "Number of skulls observed")

# Function to plot a normal distribution
standardNormal <- function() {

  # draw the plot
  xval <- seq(-3, 3, .01)
  yval <- dnorm(xval, 0, 1)
  plot(xval, yval,
    lwd = 3, ylab = "Probability Density", xlab = "Observed Value",
    frame.plot = FALSE, col = ifelse(colour, emphCol, "black"), type = "l"
  )
}
standardNormal()

# Function to plot the different probability distributions
variateRelations <- function() {

  # generate the data
  n <- 1000
  normal.a <- rnorm(n)
  normal.b <- rnorm(n)
  normal.c <- rnorm(n)
  chi.sq.3 <- (normal.a)^2 + (normal.b)^2 + (normal.c)^2
  normal.d <- rnorm(n)
  t.3 <- normal.d / sqrt(chi.sq.3 / 3)
  chi.sq.20 <- rchisq(n, 20)
  F.3.20 <- (chi.sq.3 / 3) / (chi.sq.20 / 20)

  # histogram for the normal data
  bw <- .25
  hist(normal.a, seq(min(normal.a) - bw, max(normal.a) + bw, bw),
    freq = FALSE, xlim = c(-4, 4),
    col = ifelse(colour, emphColLight, emphGrey),
    border = "white", ylim = c(0, .45), axes = FALSE,
    xlab = "", ylab = "", main = "Simulated Normal Data",
    font.main = 1
  )
  lines(x <- seq(-4, 4, .1), dnorm(x), lwd = 3, col = "black")
  axis(1)

  # histogram for the chi square data
  bw <- .5
  hist(chi.sq.3, seq(0, max(chi.sq.3) + bw, bw),
    freq = FALSE, xlim = c(0, 16),
    col = ifelse(colour, emphColLight, emphGrey),
    border = "white", axes = FALSE, ylim = c(0, .25),
    xlab = "", ylab = "", main = "Simulated Chi-Square Data",
    font.main = 1
  )
  lines(x <- seq(0, 16, .1), dchisq(x, 3), lwd = 3, col = "black")
  axis(1)

  # histogram for the t data
  bw <- .3
  hist(t.3, seq(min(t.3) - bw, max(t.3) + bw, bw),
    freq = FALSE, xlim = c(-5, 5),
    col = ifelse(colour, emphColLight, emphGrey),
    border = "white", axes = FALSE, ylim = c(0, .4),
    xlab = "", ylab = "", main = "Simulated t Data",
    font.main = 1
  )
  lines(x <- seq(-4, 4, .1), dt(x, 3), lwd = 3, col = "black")
  axis(1)

  # histogram for the F dist data
  bw <- .2
  hist(F.3.20, seq(0, max(F.3.20) + bw, bw),
    freq = FALSE, xlim = c(0, 6),
    col = ifelse(colour, emphColLight, emphGrey),
    border = "white", axes = FALSE, ylim = c(0, .7),
    xlab = "", ylab = "", main = "Simulated F Data",
    font.main = 1
  )
  lines(x <- seq(0, 6, .01), df(x, 3, 20), lwd = 3, col = "black")
  axis(1)
}
variateRelations()

# Plotting the sampling distribution of the mean with different sample sizes
samplingDistributions <- function() {

  # Plots histograms of IQ samples and sampling distributions

  plotSamples <- function(n, N) {
    IQ <- rnorm(n, 100, 15 / sqrt(N))
    hist(IQ,
      breaks = seq(10, 180, 5), border = "white", freq = FALSE,
      col = ifelse(colour, emphColLight, emphGrey),
      xlab = "IQ Score", ylab = "", xlim = c(60, 140),
      main = paste("Sample Size =", N), axes = FALSE,
      font.main = 1, ylim = c(0, .07)
    )
    axis(1)
  }

  # population distribution
  x <- 60:140
  y <- dnorm(x, 100, 15)

  # plot two different sample sizes
  plotSamples(10000, 1)
  lines(x, y, lwd = 2, col = "black", type = "l")

  # plot two different sample sizes
  plotSamples(1000, 2)
  lines(x, y, lwd = 2, col = "black", type = "l")

  # plot two different sample sizes
  plotSamples(1000, 10)
  lines(x, y, lwd = 2, col = "black", type = "l")
}
samplingDistributions()
```

## Confidence interval (CI)

```{r ci}
# computing the 2.5th and the 97.5th percentiles (95% CI) of the normal distribution
qnorm(p = c(.025, .975))

# the 15th and 85th (70% CI)
qnorm(p = c(.15, .85))

# calculating quantiles of the t distribution (95% CI)
qt(p = .975, df = 1000 - 1) # large sample
qt(p = .975, df = 10 - 1) # small sample

# calculating CI
# load data
load("~/lsr/lsr/analysis/input/afl24.Rdata")
lsr::ciMean(x = afl$attendance) # 95% CI (default)
lsr::ciMean(x = afl$attendance, conf = .99) # 99% CI (default)

# plotting CI
# using bargraph
sciplot::bargraph.CI(
  x.factor = year, # grouping variable
  response = attendance, # outcome variable
  data = afl, # data
  ci.fun = ciMean, # name of the function to calculate CIs
  xlab = "Year",
  ylab = "Average Attendance"
)

# using lineplot
sciplot::lineplot.CI(
  x.factor = year, # grouping variable
  response = attendance, # outcome variable
  data = afl, # data
  ci.fun = ciMean, # name of the function to calculate CIs
  xlab = "Year",
  ylab = "Average Attendance"
)

# using plotmeans
gplots::plotmeans(
  formula = attendance ~ year,
  data = afl,
  n.label = FALSE
)
```

## Categorical Data Analysis

```{r categorical_analysis}
# load data
load("input/randomness.Rdata")
load("input/chapek9.Rdata")
load("input/salem.Rdata")
load("input/agpp.Rdata")

#---------- The chi-squared goodnest-of-fit test ---------- 

observed <- table(cards$choice_1)
observed

# R mathematical version
probabilities <- c(clubs = .25, diamonds = .25, hearts = .25, spaces = .25)
N <- 200
expected <- N * probabilities # expected frequencies
expected

# difference between null hypothesis expected and actual values
observed - expected

# squared that difference to get a collection of numbers that are big whenever the
# null hypothesis makes a bad prediction
(observed - expected)^2

# getting different error scores
(observed - expected)^2 / expected

# getting the goodness of fit statistic (X^2 or GOF)
sum((observed - expected)^2 / expected)

# calculating the 95th percentile of a chi-squared distribution with k - 1 degrees of freedom
qchisq(p = .95, df = 3)

# getting the p-value
pchisq(q = 8.44, df = 3, lower.tail = F)
# or
1 - pchisq(q = 8.44, df = 3)

# all above calculations all at once
lsr::goodnessOfFitTest(cards$choice_1)

# specifying a different null hypothesis
nullProbs <- c(clubs = .2, diamonds = .3, hearts = .3, spades = .2)
lsr::goodnessOfFitTest(cards$choice_1, p = nullProbs)

# using chisq.test() for the goodness of fit test -> frequency table (observed)
chisq.test(observed)
chisq.test(x = observed, p = c(clubs = .2, diamonds = .3, hearts = .3, spades = .2))

#---------- The chi-squared test of independence (or association) test ---------- 

# producing a contingency table
chapekFrequencies <- xtabs(~ choice + species, data = chapek9)
chapekFrequencies
chapekFrequenciesSum <- addmargins(chapekFrequencies, margin = c(1, 2), FUN = sum, quiet = T)

# running the test
lsr::associationTest(formula = ~ choice + species, data = chapek9)

# using chisq.test() for a test of independence (assocition) -> cross-tabulation table (chapekFrequencies)
chisq.test(chapekFrequencies)

#---------- The Fisher exact test ----------

salem.tabs <- table(trial)
salem.tabs

# running the test
fisher.test(salem.tabs)

#---------- The McNemar test ----------

right.table <- xtabs(~ response_before + response_after, data = agpp)
right.table

# running the test
mcnemar.test(right.table)

cardChoices <- xtabs(~ choice_1 + choice_2, data = cards)
cardChoices

# testing if there is a relation between rows and columns
chisq.test(cardChoices)

# testing if the row totals (the frequencies for choice_1) are different from the column totals (the frequencies for choice_2)
mcnemar.test(cardChoices)
```

## Comparing two means

### The one-sample z-test

```{r z-test}
# Load data
load("~/lsr/lsr/analysis/input/zeppo.Rdata")
grades

# Calculating z-test
# 1. sample mean
sample.mean <- mean(grades)

# 2. value of the population mean null hypothesis specifies
mu.null <- 67.5

# 3. assumed population standard deviation
sd.true <- 9.5

# 4. sample size
N <- length(grades)

# 5. calculate the true standard error of the mean
sem.true <- sd.true / sqrt(N)

# 6. calculate z-score
z.score <- (sample.mean - mu.null) / sem.true
z.score

# 7. calculate critical regions (area under the curve)
# upper tail
upper.area <- pnorm(q = z.score, lower.tail = F)
upper.area
# lower tail
lower.area <- pnorm(q = -z.score, lower.tail = T)
lower.area

# 8. calculate p-value
p.value <- lower.area + upper.area
p.value
```
### The one-sample t-test

```{r t-test}
# running one-sample t-test (two sided test)
lsr::oneSampleTTest(x = grades, mu = 67.5)

# one sided test
lsr::oneSampleTTest(x = grades, mu = 67.5, one.sided = "greater")
# or
lsr::oneSampleTTest(x = grades, mu = 67.5, one.sided = "less")

# using t-test function
t.test(x = grades, mu = 67.5)
```

### The independent samples t-test

```{r indep-t-test}
# Load the data
load("~/lsr/lsr/analysis/input/harpo.Rdata")
harpo

# Parameters for plots
emphCol <- rgb(0, 0, 1)
emphColLight <- rgb(.5, .5, 1)
emphGrey <- grey(.5)
colour <- TRUE

# Plotting histograms
harpoHist <- function() {
  plotHist <- function(x, ...) {
    hist(x,
      border = "white",
      col = ifelse(colour, emphColLight, emphGrey), ...
    )
    axis(1)
  }

  # Anastasia
  plotHist(harpo$grade[harpo$tutor == "Anastasia"],
    xlim = c(50, 100), xlab = "Grade", main = "Anastasia's students",
    font.main = 1, breaks = seq(50, 100, 5), ylim = c(0, 7)
  )

  # Bernadette
  plotHist(harpo$grade[harpo$tutor == "Bernadette"],
    xlim = c(50, 100), xlab = "Grade", main = "Bernadette's students",
    font.main = 1, breaks = seq(50, 100, 5), ylim = c(0, 7)
  )
}

harpoHist()

# Plotting means and CIs
harpoMeans <- function() {
  plotmeans(
    formula = grade ~ tutor, # plot grade by test time
    data = harpo, # data frame
    n.label = FALSE, # don't show sample size
    xlab = "Class", # x-axis label
    ylab = "Grade" # y-axis label
  )

  # now add the line...
  abline(
    a = 0, # line has an intercept at 0
    b = 1 # and a slope of 1
  )
}

harpoMeans()

# running the Student's independent samples t-test (two sided test)
lsr::independentSamplesTTest(formula = grade ~ tutor, data = harpo, var.equal = T)

# running the Welch's independent samples t-test (two sided test)
lsr::independentSamplesTTest(formula = grade ~ tutor, data = harpo) # not equal variances assumed

# using t.test function (Welch's test)
t.test(formula = grade ~ tutor, data = harpo)
# Student's test
t.test(formula = grade ~ tutor, data = harpo, var.equal = T)

# one sided test
lsr::independentSamplesTTest(formula = grade ~ tutor, data = harpo, one.sided = "Anastasia")
# or
lsr::independentSamplesTTest(formula = grade ~ tutor, data = harpo, one.sided = "Bernadette")
```
### The paired-samples t-test

```{r paired-samples}
# load data
load("~/lsr/lsr/analysis/input/chico.Rdata")
chico

psych::describe(chico)
# create a "difference" variable
chico$improvement <- chico$grade_test2 - chico$grade_test1
chico

# running a paired samples t-test
#
# Option 1:
# a. create a "difference" variable (chico$improvement)
# b. run a one sample t-test on it
lsr::oneSampleTTest(chico$improvement, mu = 0)

# Option 2:
# using the pairedSamplesTTest function from lsr package (one sided formula) (two sided test)
lsr::pairedSamplesTTest(formula = ~ grade_test2 + grade_test1, data = chico)
#
# one sided test
lsr::pairedSamplesTTest(formula = ~ grade_test2 + grade_test1, data = chico, one.sided = "grade_test2")

# running a paired samples t-test from long form data
# first, format df from wide to long
chico2 <- wideToLong(chico, within = "time")
chico2

# second, using the pairedSamplesTTest (two sided formula)
# Option 1 (two sided test)
lsr::pairedSamplesTTest(formula = grade ~ time, data = chico2, id = "id")

# one sided test
lsr::pairedSamplesTTest(formula = grade ~ time, data = chico2, id = "id", one.sided = "test2")

# Option 2
lsr::pairedSamplesTTest(formula = grade ~ time + (id), data = chico2)

# using t.test function
t.test(x = chico$grade_test2, y = chico$grade_test1, paired = T) # the first element of x and y must correspond to the same person

# one sided test
lsr::pairedSamplesTTest(formula = grade ~ time + (id), data = chico2, one.sided = "test2")
```
### Effect size

```{r effect_size}
# Cohen's d from one sample
lsr::cohensD(x = grades, mu = 67.5)

# Cohen's d from a Student t-test
lsr::cohensD(formula = grade ~ tutor, data = harpo, method = "pooled")

# Cohen's d from a Welch t-test
lsr::cohensD(formula = grade ~ tutor, data = harpo, method = "unequal")
```

### Checking the normality of a sample

```{r checking_normality}
# generating data
normal.data <- rnorm(n = 100)

# plotting data
hist(x = normal.data)
qqnorm(y = normal.data)

# running a Shapiro-Wilk test
shapiro.test(x = normal.data)
```
### Testing non-normal data with Wilcoxon tests

```{r wilcoxon_tests}
# load data
load("~/lsr/lsr/analysis/input/awesome.Rdata")
load("~/lsr/lsr/analysis/input/awesome2.Rdata")
load("~/lsr/lsr/analysis/input/happy.Rdata")
awesome
score.A
score.B
happiness

# running the two sample Wilcoxon test
wilcox.test(formula = scores ~ group, data = awesome)
# for data stored separately for each group
wilcox.test(x = score.A, y = score.B)

# running the one sample Wilcoxon test
wilcox.test(x = happiness$change, mu = 0)
# or
wilcox.test(x = happiness$after, y = happiness$before, paired = T)
```

## Comparing several means (one-way ANOVA)

```{r one-way_anova}
# load data
load("~/lsr/lsr/analysis/input/clinicaltrial.Rdata")
clin.trial

# descriptive statistics
# how many people in each group
xtabs(~drug, clin.trial)

# calculating means and sd
mood.gain.mean <- aggregate(mood.gain ~ drug, clin.trial, mean) |>
dplyr::rename(mood.gain.mean = mood.gain)

mood.gain.sd <- aggregate(mood.gain ~ drug, clin.trial, sd) |>
dplyr::rename(mood.gain.sd = mood.gain)

mood.gain.mean
mood.gain.sd

# plotting the results
gplots::plotmeans(
  formula = mood.gain ~ drug,
  data = clin.trial,
  xlab = "Drug Administered",
  ylab = "Mood Gain",
  n.label = F
)

# calculating the within-group sum of squares
outcome <- clin.trial$mood.gain
group <- clin.trial$drug
gp.means <- tapply(outcome, group, mean)
gp.means <- gp.means[group]
dev.from.gp.means <- outcome - gp.means
squared.devs <- dev.from.gp.means^2
Y <- data.frame(group, outcome, gp.means, dev.from.gp.means, squared.devs)
Y
SSw <- sum(squared.devs)
SSw

# calculating the between-group sum of squares
gp.means <- tapply(outcome, group, mean)
grand.mean <- mean(outcome)
dev.from.grand.mean <- gp.means - grand.mean
squared.devs <- dev.from.grand.mean^2
gp.sizes <- tapply(outcome, group, length)
wt.squared.devs <- gp.sizes * squared.devs
Y <- data.frame(gp.means, grand.mean, dev.from.grand.mean, squared.devs, gp.sizes, wt.squared.devs)
Y
SSb <- sum(wt.squared.devs)
SSb

# calculating df (G = 3, N = 18)
df_b <- 3 - 1
df_w <- 18 - 3

# calculating the mean square values
MS_b <- 3.45 / df_b
MS_w <- 1.39 / df_w

# calculating F-value
F <- MS_b / MS_w
F

# calculating p-value
pf(F, df1 = 2, df2 = 15, lower.tail = F)

# calculating ANOVA with aov()
my.anova <- aov(mood.gain ~ drug, clin.trial)
my.anova

summary(my.anova)

# calculating the effect size (eta squared)
SStot <- SSb + SSw
eta.squared <- SSb / SStot
eta.squared
# or
etaSquared(x = my.anova)
```

### Multiple comparisons and post hoc tests

```{r multiple_comparisons}
# the tidy way
placebo <- clin.trial |>
filter(drug == "placebo") |>
dplyr::select(mood.gain)
placebo <- placebo[, 1]

anxifree <- clin.trial |>
filter(drug == "anxifree") |>
dplyr::select(mood.gain)
anxifree <- anxifree[, 1]

joyzepam <- clin.trial |>
filter(drug == "joyzepam") |>
dplyr::select(mood.gain)
joyzepam <- joyzepam[, 1]

# the non-tidy way
joyzepam <- with(clin.trial, mood.gain[drug == "joyzepam"]) # mood change due to joyzepam
anxifree <- with(clin.trial, mood.gain[drug == "anxifree"]) # mood change due to anxifree
placebo <- with(clin.trial, mood.gain[drug == "placebo"]) # mood change due to placebo

# running "pairwise" t-tests
t.test(anxifree, placebo, var.equal = TRUE) # Student t-test
# or
t.test(
  formula = mood.gain ~ drug,
  data = clin.trial,
  subset = drug %in% c("placebo", "anxifree"),
  var.equal = TRUE
)
# or
pairwise.t.test(
  x = clin.trial$mood.gain, # outcome variable
  g = clin.trial$drug, # grouping variable
  p.adjust.method = "none" # which correction to use
)
# or
lsr::posthocPairwiseT(x = my.anova, p.adjust.method = "none") # calling pairwise.t.test using an aov object

# corrections for multiple testing
# Bonferroni corrections
lsr::posthocPairwiseT(my.anova, p.adjust.method = "bonferroni")
# Holm correction
lsr::posthocPairwiseT(my.anova)
```

### Checking the homogeneity of variance assumption

```{r checking_homogeneity}
# running the Levene's test
car::leveneTest(y = my.anova, center = mean) # mean
# or
car::leveneTest(y = mood.gain ~ drug, data = clin.trial) # y is a formula in this case
# or
car::leveneTest(y = clin.trial$mood.gain, group = clin.trial$drug) # y is the outcome

# running the Brown-Forsythe test (default)
car::leveneTest(my.anova) # median
```

### Removing the homogeneity of variance assumption

```{r removing_homogeneity}
# running the Welch one-way test
oneway.test(mood.gain ~ drug, data = clin.trial)
```

### Checking the normality assumption

```{r checking_normality2}
# extract the residuals
my.anova.residuals <- residuals(object = my.anova)
my.anova.residuals

# plotting the results to check normality
hist(x = my.anova.residuals)
qqnorm(y = my.anova.residuals)
shapiro.test(x = my.anova.residuals)
```

### Removing the normality assumption

```{r removing_normality}
# running the Kruskal-Wallis test
kruskal.test(mood.gain ~ drug, data = clin.trial)
# or
kruskal.test(x = clin.trial$mood.gain, g = clin.trial$drug)
# or
mood.gain <- list(placebo, joyzepam, anxifree)
kruskal.test(x = mood.gain)
```

## Linear regression

```{r linear-regression}
# parameter to function drawBasicScatterplot
emphGrey <- grey(.5)

# plotting scatterplots (directly)
# plot(x = parenthood$dan.sleep, y = parenthood$dan.grump,
#      xlab = "Dan Sleep (hours)",
#      ylab = "Dan Grumpiness (0-100)")
# plot(x = parenthood$baby.sleep, y = parenthood$dan.grump)

# function to plot scatterplots
drawBasicScatterplot <- function(dotcol, title) {
  plot(parenthood$dan.sleep,
    parenthood$dan.grump,
    xlab = "Dan Sleep (hours)",
    ylab = "Dan Grumpiness (0-100)",
    col = dotcol,
    main = title,
    font.main = 1,
    pch = 19
  )
}

drawBasicScatterplot(emphGrey, "")

# plotting a depiction of the residuals associated with the best fitting regression line
# good regression line
drawBasicScatterplot(emphGrey, "Regression Line Close to the Data")
good.coef <- lm(dan.grump ~ dan.sleep, parenthood)$coef
abline(good.coef, col = ifelse(colour, emphCol, "black"), lwd = 3)
for (i in seq_along(parenthood$dan.sleep)) {
  xval <- parenthood$dan.sleep[i] * c(1, 1)
  yval <- c(parenthood$dan.grump[i], good.coef[1] + good.coef[2] * parenthood$dan.sleep[i])
  lines(xval, yval, type = "l", col = emphGrey)
}

# bad regression line
drawBasicScatterplot(emphGrey, "Regression Line Distant from the Data")
bad.coef <- c(80, -3)
abline(bad.coef, col = ifelse(colour, emphCol, "black"), lwd = 3)
for (i in seq_along(parenthood$dan.sleep)) {
  xval <- parenthood$dan.sleep[i] * c(1, 1)
  yval <- c(parenthood$dan.grump[i], bad.coef[1] + bad.coef[2] * parenthood$dan.sleep[i])
  lines(xval, yval, type = "l", col = emphGrey)
}

# running a simple regression model
regression.1 <- lm(dan.grump ~ dan.sleep, data = parenthood)
regression.1

# running a multiple linear regression
regression.2 <- lm(dan.grump ~ dan.sleep + baby.sleep, data = parenthood)
regression.2
```
### Quantifying the fit of the regression model

```{r R-squared}
# The R-squared value
X <- parenthood$dan.sleep # the predictor
Y <- parenthood$dan.grump # the outcome
Y.pred <- -8.94 * X + 125.97

# calculating the sum of squared residuals
SS.resid <- sum((Y - Y.pred)^2)
SS.resid

# calculating the total sum of squares
SS.tot <- sum((Y - mean(Y))^2)
SS.tot

# calculating the coefficient of determination or R-squared
R.squared <- 1 - (SS.resid / SS.tot)
R.squared

# relation between regression and correlation
# calculation correlation (r-Pearson correlation coefficient)
r <- cor(X, Y)
r^2
```
### Hypothesis test for regression models

```{r hypothesis-test-regression}
# compute all the quantities of the regression model
summary(regression.2)
```
### Hypothesis test for a single correlation

```{r hypothesis-test-single-correlation}
# running t-test on a coefficient in a regression model
summary(regression.1)
# is the same
cor.test(x = parenthood$dan.sleep, y = parenthood$dan.grump)
```
### Regarding regression coefficients

```{r regression-coefficients}
# confidence intervals for the coefficients
confint(object = regression.2, level = .99)

# calculating standardised regression coefficients
lsr::standardCoefs(regression.2)
```
### Model checking

```{r model-checking}
# Residuals
# getting ordinary residuals
residuals(object = regression.2)

# getting standardised residuals
rstandard(model = regression.2)

# getting Studentised residuals
rstudent(model = regression.2)

# Anomalous data
# getting hat values
hatvalues(model = regression.2)

# getting Cook's distance values
cooks.distance(model = regression.2)

# test from car package to see if any of the Studentised residuals are significantly larger
car::outlierTest(model = regression.2)

# plotting anomalous data using plot()
plot(x = regression.2, which = 4) # Cook's distance 
plot(x = regression.2, which = 5) # the Studentised residuals agains leverage

# using the car package
car::influenceIndexPlot(model = regression.2)
car::influencePlot(model = regression.2)

# getting rid of outliers using subset
lm(formula = dan.grump ~ dan.sleep + baby.sleep, data = parenthood, subset = -64)

# Checking the normality of the residuals
# plotting histogram
hist(x = residuals(regression.2),
     xlab = "Value of residual",
     main = "",
     breaks = 20)

# running a Shapiro-Wilk test
shapiro.test(x = residuals(regression.2))

# plotting qq-plot
plot(x = regression.2, which = 2)

# Checking the linearity of the relationship
# plotting the relationship between the fitted values and the observed values
yhat.2 <- fitted.values(object = regression.2)
plot(x = yhat.2,
     y = parenthood$dan.grump,
     xlab = "Fitted Values",
     ylab = "Observed Values")

# plotting the relationship between the the fitted values and the residuals themselves
plot(x = regression.2, which = 1)
# with residualPlots() function from car package
car::residualPlots(model = regression.2)

# Checking the homogeneity of variance
plot(x = regression.2, which = 3)

# running the non-constant variance test
car::ncvTest(regression.2)

# running sandwich estimators if homogeneity of variance is violated
lmtest::coeftest(regression.2, vcov. = hccm)

# Checking for collinearity
# calculating the variance inflation factors (VIFs)
car::vif(mod = regression.2)

# example of getting collinearity
regression.3 <- lm(day ~ baby.sleep + dan.sleep + dan.grump, parenthood)
car::vif(mod = regression.3)
```

### Model selection

```{r model-selection}
# the backward elimination approach
# 1. get the complete regression model
full.model <- lm(formula = dan.grump ~ dan.sleep + baby.sleep + day, data = parenthood)

# 2. running for the first time the step() function: backward
step(object = full.model, direction = "backward")

# running step() forward
null.model <- lm(dan.grump ~ 1, parenthood)
step(object = null.model, direction = "forward", scope = dan.grump ~ dan.sleep + baby.sleep + day)

# comparing two regression models
# 1. running the regressions
M0 <- lm(dan.grump ~ dan.sleep + day, parenthood)
M1 <- lm(dan.grump ~ dan.sleep + day + baby.sleep, parenthood)

# A. based on a model selection criterion (AIC)
AIC(M0, M1)

# B. calculating the F statistic
anova(M0, M1)
```
## Comparing several means with more than one grouping variable (factorial or two-way ANOVA)
### Factorial ANOVA 1: balanced designs 

```{r factorial-anova}
# cross-tabulating variables
xtabs(~ drug + therapy, clin.trial)

# calculating a cross-tabulation of the group means for all possible combinations
# for factor drug
aggregate(mood.gain ~ drug, clin.trial, mean)
# for factor therapy
aggregate(mood.gain ~ therapy, clin.trial, mean)
# for both
total_means <- aggregate(mood.gain ~ drug + therapy, clin.trial, mean)
# for none (the mean)
mean(clin.trial$mood.gain)

# running aov using only a single factor
model.1 <- aov(mood.gain ~ drug, clin.trial)
summary(model.1)

# running aov using two grouping factors
model.2 <- aov(mood.gain ~ drug + therapy, clin.trial)
summary(model.2)

# plotting to check interaction
sciplot::lineplot.CI(x.factor = clin.trial$drug,
            response = clin.trial$mood.gain,
            group = clin.trial$therapy,
            ci.fun = ciMean,
            xlab = "drug",
            ylab = "mood gain")

# running factorial ANOVA with interaction
model.3 <- aov(mood.gain ~ drug * therapy, clin.trial)
# or
# model.3 <- aov(mood.gain ~ drug + therapy + drug:therapy, clin.trial)
summary(model.3)

# calculating effect size
lsr::etaSquared(model.2)

# calculating effect size with interaction
lsr::etaSquared(model.3)

# calculating estimates of all group means
eff <- effects::effect(term = "drug*therapy", mod = model.3)
eff

# extracting confidence intervals
summary(eff)

# no interaction
eff2 <- effects::effect(term = "drug*therapy", mod = model.2)
eff2
summary(eff2)
```

#### Assumption checking

```{r assumpiton-checking}
# Levene test for homogeneity of variance
car::leveneTest(mood.gain ~ drug * therapy, clin.trial)

# testing the normality of the residuals
# 1. extracting the residuals
resid <- residuals(model.2)

# 2. plotting the residuals
hist(resid)
qqnorm(resid)

# 3. running the Shapiro-Wilk test
shapiro.test(resid)

# running F-test for model comparison
anova(model.1, model.3)
```
#### ANOVA as linear model

```{r anova-as-linear}
# load data
load("~/lsr/lsr/analysis/input/rtfm.Rdata")
rtfm.1
rtfm.2

# running an ANOVA analysis
anova.model <- aov(grade ~ attend + reading, data = rtfm.2)
summary(anova.model)

# taking notes of F statistics (the observed group means)
effects::Effect(c("attend", "reading"), anova.model)

# running a regression
regression.model <- lm(grade ~ attend + reading, data = rtfm.1)
coef(summary(regression.model))

# running ANOVA function to a regression model
anova(regression.model)

# encoding non binary factors as contrasts
clin.trial$druganxifree <- as.numeric(clin.trial$drug == "anxifree")
clin.trial$drugjoyzepam <- as.numeric(clin.trial$drug == "joyzepam")
# or using the expandFactors() function
clin.trial2 <- lsr::expandFactors(clin.trial)

# running the ANOVA model in a three-level factor data frame
drug.model <- aov(mood.gain ~ drug + therapy, data = clin.trial)
summary(drug.model)

# running a regression model in a two binary contrasts data frame
drug.regression <- lm(mood.gain ~ druganxifree + drugjoyzepam + therapyCBT, data = clin.trial2)
summary(drug.regression)

# running the F-test
nodrug.regression <- lm(mood.gain ~ therapyCBT, data = clin.trial2)
anova(nodrug.regression, drug.regression)

# running a regression model in a three-level factor data frame
drug.lm <- lm(mood.gain ~ drug + therapy, data = clin.trial)
summary(drug.lm)

# using anova() gives us an ANOVA table
anova(drug.lm)

# creating treatment contrasts
contr.treatment(n = 5)

# last category treated as the baseline
contr.SAS(n = 5)
# or
contr.treatment(n = 5, base = 5)

# creating a contrast matrix with Helmert contrasts
contr.helmert(n = 5)

# creating "sum to zero" matrix
contr.sum( n = 5)

# checking contrasts matrix for a df
attr(clin.trial$drug, "contrasts")
contrasts(clin.trial$drug) # per default: contr.treatment()

# setting the contrasts for a single factor
contrasts(clin.trial$drug) <- contr.sum(n = 3)
contrasts(clin.trial$drug)
contrasts(clin.trial$drug) <- NULL # revert the defaults
contrasts(clin.trial$drug)

# setting the contrasts for a single analysis
# 1. creating a list with the contrasts types
my.contrasts <- list(drug = contr.helmert, therapy = contr.helmert)

# 2. fitting the ANOVA model with the contrasts argument specified
mod <- aov(mood.gain ~ drug * therapy, clin.trial, contrasts = my.contrasts)

# 3. checking
mod$contrasts
```
#### Post hoc tests

```{r post-hoc-tests}
# running Tukey's HSD
TukeyHSD(model.2)

# with interactions
TukeyHSD(model.3)
```
### Factorial ANOVA 2: unbalanced designs

```{r unbalanced-designs}
# load data
load("~/lsr/lsr/analysis/input/coffee.Rdata")

# calculating means between different groups
aggregate(babble ~ milk + sugar, data = coffee, FUN = mean)

# calculating sd between different groups
aggregate(babble ~ milk + sugar, data = coffee, FUN = sd)

# checking how many observations in each group
xtabs(~ milk + sugar, data = coffee)

# running Type I sum of squares
mod.type1 <- lm(babble ~ sugar + milk + sugar:milk, data = coffee)
anova(mod.type1)
# or
mod.1 <- lm(babble ~ 1, data = coffee)
mod.2 <- lm(babble ~ sugar, data = coffee)
mod.3 <- lm(babble ~ sugar + milk, data = coffee)
mod.4 <- lm(babble ~ sugar + milk + sugar:milk, data = coffee)
anova(mod.1, mod.2, mod.3, mod.4)

# the order matters
mod_order <- lm(babble ~ milk + sugar + sugar:milk, data = coffee)
anova(mod_order)

# running Type III sum of squares (default contrasts = treatment contrasts)
mod.type3 <- lm(babble ~ sugar * milk, data = coffee)
car::Anova(mod.type3, type = 3)

# changing the contrasts
my.contrasts.type3 <- list(milk = "contr.Helmert", sugar = "contr.Helmert")
mod.H <- lm(babble ~ sugar * milk, data = coffee, contrasts = my.contrasts.type3)
car::Anova(mod.H, type = 3)

# running Type II sum of squares
mod.type2 <- lm(babble ~ sugar * milk, data = coffee)
car::Anova(mod.type2, type = 2)

# calculating eta squares and partial eta squares for unbalanced designs and for different Types of tests
es <- lsr::etaSquared(mod.type1, type = 2, anova = TRUE)
es
```
## Bayesian statistics

### Bayesian analysis of contingency tables

```{r bayesian-statistics}
# load data
load("~/lsr/lsr/analysis/input/chapek9.Rdata")

# running cross-tabulation
crosstab <- xtabs(~ species + choice, data = chapek9)
crosstab

# running a chi-square test of association
lsr::associationTest(~ species + choice, data = chapek9)

# running the analysis of the contingency table the Bayesian way (joint multinomial sampling)
BayesFactor::contingencyTableBF(crosstab, sampleType = "jointMulti")

# with Poisson sampling plan
BayesFactor::contingencyTableBF(crosstab, sampleType = "poisson")

# with independent multinomial plan
BayesFactor::contingencyTableBF(crosstab, sampleType = "indepMulti", fixedMargin = "rows")

# with experimental design plan (just for small contingency tables = 2X2)
# BayesFactor::contingencyTableBF(crosstab, sampleType = "hypergeom")
```

### Bayesian t-tests

#### Independent samples t-test 

```{r bayesian-t-tests-independent}
# running an independent samples t-test
lsr::independentSamplesTTest(formula = grade ~ tutor, data = harpo, var.equal = TRUE)

# running an independent samples t-test the Bayesian way
BayesFactor::ttestBF(formula = grade ~ tutor, data = harpo)
```
#### Paired samples t-test

```{r bayesian-t-tests-paired}
# running an paired samples t-test the Bayesian way
BayesFactor::ttestBF(x = chico$grade_test1, y = chico$grade_test2, paired = TRUE)
```
### Bayesian regression

```{r bayesian-regression}
# running regression the Bayesian way
BayesFactor::regressionBF(formula = dan.grump ~ dan.sleep + day + baby.sleep, data = parenthood)

# narrowing the models
models <- BayesFactor::regressionBF(formula = dan.grump ~ dan.sleep + day + baby.sleep, data = parenthood)
head(models, n = 3)

# knowing the difference between best models
head(models / max(models), n = 3)

# calculating the difference between the two best models (row 1 / row 4 in the original output)
models[1] / models[4]

# specifying a preferred model
BayesFactor::regressionBF(formula = dan.grump ~ dan.sleep + baby.sleep, data = parenthood, whichModels = "top")
```
### Bayesian ANOVA

```{r bayesian-anova}
# running ANOVA the Bayesian way
modelsANOVA <- anovaBF(formula = mood.gain ~ drug * therapy, data = clin.trial)
modelsANOVA

# identifying the best model
modelsANOVA / max(modelsANOVA)

# getting BF for a Type II ANOVA
max(modelsANOVA) / modelsANOVA
```
# Linting

The code in this RMarkdown is linted with the [lintr package](https://github.com/jimhester/lintr), which is based on the [tidyverse style guide](http://style.tidyverse.org/).

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
# lintr::lint("main.Rmd", linters =
#               lintr::with_defaults(
#                 commented_code_linter = NULL,
#                 trailing_whitespace_linter = NULL
#                 )
#             )
# if you have additional scripts and want them to be linted too, add them here
# lintr::lint("scripts/my_script.R")
```
